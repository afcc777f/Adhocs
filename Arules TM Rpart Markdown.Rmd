---
title: "Applied Machine Learning"
author: "Adrian Cuyugan | Information Analytics, GAMBI"
output:
  pdf_document:
    fig_caption: yes
    fig_width: 7
    number_sections: yes
    toc: yes
    toc_depth: 3
  word_document: default
subtitle: Text Retrieval, Association Rules and Decision Tree Learning using R
email: acuyugan@csc.com
---

------

# Introduction

Usually, Machine Learning stops after you understand how the algorithm runs. Data Mining, on the other, is the real-life application of how these complex algorithms is put into practice with very noisy data - recognizing any patterns, discovering outliers and estimating coefficients which and can later be developed to produce accurate predictions.

# Scope

## Techniques

This document assumes that you have basic knowledge on the following subjects:

1. **Information Retrieval** - the lesson stops at text retrieval since the data is often offline and does not necessarily require a contant query function, $f(q,d)$. For this activity, [tm](http://cran.r-project.org/web/packages/tm/tm.pdf) package is used to create a corpus and build the sparse matrix. Also, n-gram bag of words is used to construct our sparse matrix using the raw term frequency count for the association rules and a binary-term frequency weighting for decision tree learning.
$$VSM(q,d)=\sum_{i=1}^{n}x_{1}y_{1}...+...x_{n}y_{n}$$

2. **Association Rules** - finding frequent patterns based on the text corpus we have built from tm. [arules](http://cran.r-project.org/web/packages/arules/arules.pdf) and [arulesViz](http://cran.r-project.org/web/packages/arulesViz/arulesViz.pdf) are used for this. Also, [Gephi](http://gephi.github.io/) visualizes the network of rules created from the Apriori algorithm. *Corpus* is a latin word which is a collection of plain text that can be read by the computer when doing Natural Language Processing - linguistics, building sparse matrix, frequency count, etc.

3. **Decision Tree** - the simplest Classification and Regression Tree (CART) algorithm computes for entropy based the response variable that we set, *sentiment*. Remember that entropy is:
$$E=-\sum_{i=1}^{k}\frac{C_{i}}{n}log_{2}\frac{C_{i}}{n}$$
[rpart](http://cran.r-project.org/web/packages/rpart/rpart.pdf) builds CART and estimates the model because it contains additional features like cross-validation within training set. Also, [rattle](http://cran.r-project.org/web/packages/rattle/rattle.pdf) is used to visualize the tree. 

For beginners, **rattle** is another data mining tool in R which has a GUI if you are not yet familiar with R coding. 

**Warning**: There are so many data analysts who can use the tool but does not know the algorithm behind it. This is a danger zone because you can interpret the results incorrectly.

## Before starting

You can use `?help` to search what are the description and available parameters that you can use in this example. For example, if you want to know what does `head()` do, type `?head` in   R console.

The scripts are coded as simple as possible with attached comments so that you can understand what happens within each line of code.

The scripts do not include `install.packages()` as it assumes that you have already installed the packages. If you have not, please refer to the above techniques as the names of packages are highlighted with their corresponding documentation from CRAN directory.

# Data 

## Source and Goal

A current knowledge-based competition (bragging rights award) is being held at [Kaggle](https://www.kaggle.com). One of the available data set we could use is [Google's Word2Vec's](https://www.kaggle.com/c/word2vec-nlp-tutorial). For this exercise, Natural Language Processing (parsing of words) is not done because it requires further understanding on the subject matter, only bag of words.

Although model evaluation is not discussed (though I highly suggest that you make further reading on this), model accuracy and other quantitative measures are included. There is a big difference between building the model and interpreting the results than doing a prediction that leads to prescribing recommendations as these models are tested against another source of data set which is usually the testing set and/or cross-validation set.

## Bag of Words

Unigram *bag of words* is used to create the vector of words based on all 25,000 movie reviews. These are also labelled so that we can calculate the estimates of probability of a word producing a positive or negative review. Sampling is also done to produce the association rules as it requires more memory to run the algorithm.

## Data fields

1. **id** - Unique ID of each review. This is excluded from the dataset but is very helpful in identifying the unique records of review. `rowname()` function in R accomplishes this task.
2. **sentiment** - The response variable. This is the sentiment of the review; 1 for positive reviews and 0 for negative reviews. This is ignored when doing unsupervised learning.
3. **review** - This is a free-form text field where the users wrote their reviews. This is converted into a *Document Term Matrix* as a result of the text retrieval process.

# Text Retrieval

## Load the dataset

The CSV file is downloaded for offline use so that this document can be generated faster. The data set could be saved into the working directory. You can check your current working using `getwd()` or manually set it using `setwd()`. For easier viewing of the codes, C drive is used as a working directory.

```{r Load the dataset, warning=FALSE}
train <- read.csv("C://popcorn-train.csv", head = T, sep = ",") # Load data set

rownames(train) <- train$id # Assigns the ID as row identifier
train <- train[,2:3] # Subsets only Sentiment and Comments
train$sentiment <- factor(train$sentiment, 
                          levels = c(1,0), 
                          labels = c("Pos", "Neg")) # Converts 0 and 1 into categories
table(train$sentiment) # Count of records per sentiment
```

After loading the data set into R environment, **id** variable is assigned as the row identifier then, it is dropped since it has already been attached. Since R automatically recognizes **sentiment** as a numerical variable, this is changed to categorical by using the `factor()` function. 

Note that the proportion of Positive and Negative reviews are equal.

## Build corpus

The corpus could be built from a data frame or a vector format. This example uses data frame since the CSV have been loaded as a data frame. Each line of code converts the corpus into its respective transformation function. For more advanced control of parameters, please refer to `?tm_map`.

```{r Build the corpus, warning=FALSE, message=FALSE}
# Prepare corpus
require(tm)
keyword <- as.data.frame(train$review)
keyword <- Corpus(DataframeSource(keyword)) # Creates corpus
keyword <- tm_map(keyword, removeWords, stopwords(kind = "english")) # Removes stop words
keyword <- tm_map(keyword, stemDocument, language = "porter") # Applies porter stemming
keyword <- tm_map(keyword, stripWhitespace) # Removes whitespace
keyword <- tm_map(keyword, content_transformer(tolower)) # Converts to lowercase
keyword <- tm_map(keyword, removePunctuation) # Removes punctuation marks
keyword <- tm_map(keyword, removeNumbers) # Removes numbers
```

```{r Raw term frequency, warning=FALSE}
# Raw Term Frequency
keyword.term <- DocumentTermMatrix(keyword) # Creates a sparse matrix
keyword.term # Checks for how many terms were produced
```

There are $102,984$ terms produced by the algorithm. In Information Retrieval, all of these could be included in the model but since it built a very large matrix, dropping sparsed terms that barely have frequency count in the matrix is done to minimize computational load.

## Sparsity

Use `removeSparseTerms` to remove low frequency count terms.

```{r Sparsity, warning=FALSE}
keyword.term <- removeSparseTerms(keyword.term, sparse = 0.9)
keyword.term # Checks for sparsity
```

After sparsing, non-essential terms is minimized to < 130 or 80%. Although this is still a lot of variables that CART would handle, based on a data mining analyst's judgement, this could still be adjusted to < 0.90 which would lessen the bag of words/variables included in the final model.

As CART requires a response variable, the document term matrix is combined with the sentiment vector.

```{r Matrix, warning=FALSE}
keyword.matrix <- as.data.frame.matrix(keyword.term) # Converts to data frame matrix
keyword.matrix <- cbind(keyword.matrix, sentiment = train$sentiment) # Combines response
rownames(keyword.matrix) <- row.names(train) # Applies row identifier
head(keyword.matrix[150:170,118:127])# Previews the combined data frame
```

# Association Rules

## Negative reviews

For this activity, only the negative reviews are used to look for frequent patterns. Note that when the matrix is built, raw term frequency is used; the values of the matrix would be totally different if TF-IDF weighting is selected.

## Random sampling

Building models takes a lot of memory resources and when you hit the ceiling, your computer will definitely crash. To minimize the computational load, the negative reviews is sampled without replacement to 5,000 records.

```{r Negative reviews, warning=FALSE}
popcorn.neg <- subset(keyword.matrix, sentiment == "Neg") # Subsets negative reviews
popcorn.neg <- popcorn.neg[, !(colnames(popcorn.neg) %in% "sentiment")] # Drops response

# Random sampling
set.seed(14344) # For reproducability
popcorn.neg <- popcorn.neg[sample(1:nrow(popcorn.neg), 5000, replace=FALSE),] # Samples
popcorn.neg <- as.matrix(popcorn.neg) # Converts data frame to matrix
```

For the results to be fully reproducible, `set.seed()` is used so that when you generate the resampling function, the same records are selected.

## A Priori algorithm

One of the most common algoritms in doing association rules is a priori. Before running the function, the data set needs to be converted from matrix/data frame into a transactions class. For example:

ID | Transactions/Bag of Words
---|-------------------------------------------------------------:
1  | example, film, interested, just, like, look, movie, scene, will
2  | because, beyond, director, movie, know, like, movie, want
3  | because, better, forget, get, know, look, movie, not, will, work
4  | better, interested, movie, same, something, work

```{r Transaction, warning=FALSE, message=FALSE}
require(arules)
popcorn.neg <- as(object = popcorn.neg, Class = "transactions") # Converts to transaction
popcorn.neg
```

After sampling, a reproducible random sample of 5,000 records with 127 bag of words is converted into a transactions matrix format like above.

```{r apriori, warning=FALSE}
neg.arules <- apriori(popcorn.neg, parameter=list(support=0.1, conf=0.1)) # Runs algorithm
neg.arules <- sort(neg.arules, decreasing=T, by="lift") # Sorts based on lift values
```

The $minsup$ (*relative support*) and $minconf$ (*conditional probability*) values totally depend on the analyst's judgement; also, expertise in the domain helps. Confidence is computed as follows:
$$conf(X\Rightarrow Y)=\frac{sup(X\cup Y)}{sup(x)} = P(Y|X) = \frac{P(Y\cap X)}{P(X)}$$

Find time to look at the output of the model. Based on the $minsup = 10\%$ and $minconf = 10\%$, there are 3,099 rules that are discovered. The $minlen$ could also be tweaked. 

Then, the rules are sorted based on the magnitude of lift values. Lift is computed as:
$$lift(X,Y)=\frac{conf(X\Rightarrow Y)}{sup(Y)}=\frac{sup(X\cup Y)}{sup(X)*sup(Y)}$$

## Subset prune
One property of frequent patterns is the downward closure where an itemset subset is also frequent if the parent itemset is also frequent. Pruning resolves this to remove ambiguity in the rules. 

```{r Pruning, warning=FALSE}
neg.prune <- is.subset(x = neg.arules, y = neg.arules) # Finds subsets
neg.prune[lower.tri(x = neg.prune, diag = TRUE)] <- NA # Discards lower triangle of matrix
neg.redundant <- colSums(neg.prune, na.rm = TRUE) >= 1 # Computes for sums of terms
neg.prune <- neg.arules[!neg.redundant] # Removes redundancies based on subsets

# Converts rules into data frame
# elements is a sub-class of the rule, just like subsetting
neg.arules.df = data.frame(
  lhs = labels(lhs(neg.prune))$elements,
  rhs = labels(rhs(neg.prune))$elements, 
  neg.prune@quality)

head(neg.arules.df, 20) # Views top 20 rules based on lift
neg.prune # Total rules after pruning
```

You can use `inspect()` to view the rules from the arules package. Unfortunately, since the tm package is loaded in the environment and it uses the same function for a different purpose, this masks `inspect()` from arules package. That's why the rules are converted into a data frame so that it can be easily exported or viewed.

There are a total of 1,249 rules after pruning.

Going back to the top 20 rules, assume interpretations of patterns found by the a priori algorithm.

## Custom stopwords

Now, the goal is to construct associations based on the rules found. As the rules that merit high lift values do not make sense (or maybe they do, again, domain expertise is needed), *just, the, really*, these can be added to a list of custom stop words. Then, another run of pattern mining could be done to find more meaningful words.  For this exercise, let's assume that these words make sense to construct meaningful assumptions or **if** $\Rightarrow$ **then** statements.

Another approach is to use TF-IDF because common words such as *movi* and *movie* score low using this weighting. Remember that a normalized TF-IDF with smoothing is computed as:
$$TF_{IDF}=log[\frac{(M+1)}{k}]$$

## Graph

The first 40 rules ranked by the magnitude of the lift value are visualized to show their association. The size of the bubbles is the $support$ and the color density of it is the $lift$. Precision is set to show upto tenth decimal and cex is a numerical value giving the amount by which plotting text and symbols should be magnified/demagnified relative to the default.

```{r Items and itemsets, warning=FALSE, message=FALSE, fig.cap="Items and itemsets"}
require(arulesViz)
par(mfrow=c(1,2)) # Displays two plots in one graphics
plot(head(neg.prune,40), method="graph",
     control=list(type="items", precision=1, cex=.6, 
                  main="Negative Reviews Items")) # Plots items rules
plot(head(neg.prune,40), method="graph", 
     control=list(type="itemsets", precision=1, cex=.6, 
                  main="Negative Reviews Itemsets")) # Plots itemsets
par(mfrow=c(1,1)) # Resets graphic device
```

Using some basic understanding in graph theory (which is another topic - centrality, degrees, clustering, etc), the words could be clustered based on their association with one another. I would let you interpret the graphs for deeper understanding.

Another approach is to perform community modularity clustering based on all rules found in the model. [Gephi](http://gephi.github.io/) is particularly effective in this and it is open-source. Figure 2 is an example.

Basic NLP could be performed by just looking for patterns using *syntactic parsing* (nouns vs verbs) compared with positive reviews. By parsing the bag of words using bigram to n-gram models, more advanced techniques such as *Latent Dirichlet Allocation* to perform [Topic Modeling](https://cpsievert.shinyapps.io/newsgroup/) could be done. This is very helpful in classifying words specially when analyst's domain expertise is very limited.

```{r Text Network Analysis, warning=FALSE, fig.cap="Text Network Analysis using Gephi", echo=FALSE}
require(png, quietly = T)
require(grid, quietly = T)
tna <- readPNG("C://PopCorn Neg.png")
grid.raster(tna)
```

# Decision Tree Learning

## Frequent words

As mentioned, a typical CART is a greedy algorithm that prefers to look for lower entropy in certain variables that contain higher proportion compared to the rest. Another option before building the model is to look at the most frequent words that are found from the term matrix.

```{r Most frequent words, warning=FALSE}
findFreqTerms(keyword.term, lowfreq = 5000) # Which words are most frequent?
```

## Bit-vector weight

Assume that you are not familiar with the domain (analyzing movie reviews from forum, for this case), all of the the variables could be used as predictor. 

```{r Bit-vector weighting, warning=FALSE}
keyword.binary <- weightBin(keyword.term) # Applies binary transformation
keyword.binary <- as.data.frame.matrix(keyword.binary) # Converts to data frame
keyword.binary <- cbind(keyword.binary, sentiment = train$sentiment) # Combines response
keyword.binary <- as.data.frame(sapply(keyword.binary, 
                                       FUN = as.factor)) # Converts columns to categorical
rownames(keyword.binary) <- row.names(train) # Applies unique ID
```

To further simplify the model building, bit-vector weighting is used. As the matrix contains 1 and 0, all predictors are converted to categorical variables to signify missing and present in the document.

## Training and testing sets

Using complex algorithms to perform descriptive statistics tends to be useless if this cannot be used to predict future state. The simplest way to test accuracy of the model is to divide the data set into two: training and test sets; typically, it is divided into 80-20 splits.

```{r Train/test set, warning=FALSE}
set.seed(14344) # For reproducibility 
split <- sample(seq_len(nrow(keyword.binary)), # Counts records from 1 to n
                size = 0.8 * nrow(keyword.binary), # Computes for 80% for training set
                replace = FALSE)

popcorn.train <- keyword.binary[split, ] # Creates training set
popcorn.test <- keyword.binary[-split, ] # Created testing set
```

Other methods like bootstrapping and k-fold validations could be used to make sure that the model is iteratively tested with the lowest standard error as possible.

## Build and visualize tree

```{r Build tree, warning=FALSE, message=FALSE}
require(rpart)
require(rattle)
popcorn.tree <- rpart(formula = sentiment ~ ., # tests all predictors against response
                      data = popcorn.train, # From the training set
                      method = "class", # Tells model it is a classification tree
                      parms = list(split = "information"), # Uses information gain
                      model = T) # Retains model information
```

The line that indicates "*" denotes a terminal node of the tree (i.e., a leaf node - the tree is not split any further at that node). The legend is also printed. I highly recommend that you interpret the results based on the output with the visualized tree. You can start from the root (top-down approach) or from the leaf (bottom-up approach.)

Information gain is used to a control parameter when the algorithm converges. This is represented as:
$$IG=E_{\mu}(T)-E_{\mu}(T,a)$$

```{r CP, warning=FALSE, fig.cap="Decision Tree"}
# Prints nodes and leaves
print(popcorn.tree)

# Sets side by side layout of the two plots
layout(matrix(c(1,2,3,4), nrow = 1, ncol = 2, byrow = TRUE), widths=c(2.5,2)) 

# Plots the model
fancyRpartPlot(model = popcorn.tree, main = NULL, sub = NULL) 

# Plots variable importance
barplot(popcorn.tree$variable.importance, 
        cex.names = 0.5, 
        sub = "Variable Importance") 
popcorn.tree$variable.importance
```

Now, interpret the tree. This is explained in the active training sessions.

## Complexity parameter

The complexity parameter $\alpha$ specifies how the cost of a tree $C(T)$ is penalized by the number of terminal nodes $\left | T \right |$, resulting in a regularized cost $C\alpha(T)$. This is not discussed the previous lessons as this is only applicable when you perform k-fold cross-validations. The package performs 10 folds as a default. This further tests the accuracy of the model even without a testing set. Note the misclassification error after cross-validation with the error rate when we evaluate against the testing set later.

This is represented as:
$$C \alpha (T) = C(T) + \alpha \left | T \right |$$

```{r Complexity parameter, warning=FALSE}
printcp(popcorn.tree) # Prints CP
```

In a nutshell the: misclassification error (33.13%) = root node error (49.96%) * relative error of lowest CP (66.313%). More statistics are available in `summary(popcorn.tree)`. 

## Model evaluation

With a good accuracy rate, typically above 80%, the model can be considered to perform good predictions. In machine learning and diagnostics, [F1-score](http://en.wikipedia.org/wiki/F1_score) is considered as one the basic quantitative measures to describe how good the model is in terms of trade-off between precision and recall; this is calculated as:
$$F_{1}= 2 \tfrac{precision * recall}{precision + recall}$$

To expand the formula to show precision and recall:
$$F_{1}= 2 \tfrac{\frac{TP}{(TP + FP)} * \frac{TP}{(TP + FN)}}{(\frac{TP}{(TP + FP)} + \frac{TP}{(TP + FN)})}$$

In a classification model evaluation, precision and recall always come together, hence the F-measure. Wikipedia has a good example of this in [information retrieval](http://en.wikipedia.org/wiki/Precision_and_recall) domain. 

```{r Model evaluation, warning=FALSE}
popcorn.prediction <- predict(object = popcorn.tree, # Tests model
                              newdata = popcorn.test, # with Testing set
                              type = "class") # Tells it is a classification prediction

# Build binary classification confusion matrix
popcorn.confusion <- table(Actual = popcorn.test$sentiment, 
                           Predicted = popcorn.prediction)

tp <- popcorn.confusion[1,1] # True Positive
tn <- popcorn.confusion[2,2] # True Negative
fp <- popcorn.confusion[2,1] # False Positive / Type I error
fn <- popcorn.confusion[1,2] # False Negative / Type II error
n <- sum(popcorn.confusion) # Total records of testing set

popcorn.accuracy <- (tp + tn) / n # Accuracy rate = 66.78%
popcorn.error <- (fp + fn) / n # Error rate = 33.22%
popcorn.precision <- tp / (tp + fp) # Precision / Sensitivity = 69.50%
popcorn.recall <- tp / (tp + fn) # Recall / Specificity = 60.21%
popcorn.f1 <- 2 * popcorn.precision * popcorn.recall / 
  (popcorn.precision + popcorn.recall) # F1-score / Harmonic mean
popcorn.oddsratio <- (tp * tn) / (fp * fn) # Odds ratio = 4 folds
```

Remember the misclassification rate from the cross-validation? It is very close to the error rate when the model is tested using the testing set. I encourage you to interpret the other measures. 

# Citations

## References

Jiwawei Han, Micheleine Kamber and Jian Pei, *Data Mining: Concepts and Techniques*. Morgan Kaufmann, 3rd ed., 2011

Chary Aggarwal and Jiawei Hand (eds.), *Frequent Patter Mining*, Springer 2014

Chris Manning and Hinrich Schütze, *Foundations of Statistical Natural Language Processing*, MIT Press. Cambridge, MA: May 1999.

Mark Sanderson. *Test Collection Based Evaluation of Information Retrieval Systems.* Foundations and Trends in Information Retrieval 4(4): 247-375 (2010).

Zhai, ChengXiang. *Statistical Language Models for Information Retrieval. Synthesis Lectures Series on Human Language Technologies*. Morgan & Claypool Publishers, 2008.

Wil van der Aalst. *Process Mining: Discovery, Conformance and Enhancement of Business Processes*, Springer Verlag, 2011, ISBN 978-3-642-19344-6

Angela Chapman. Word2Vec - Google Code. Kaggle's competition for using Google's word2vec package for sentiment analysis.

Xie Y (2013). _Dynamic Documents with R and knitr_. Chapman and Hall/CRC, Boca Raton,
Florida. ISBN 978-1482203530, <URL: http://yihui.name/knitr/>.

Xie Y (2014). “knitr: A Comprehensive Tool for Reproducible Research in R.” In Stodden
V, Leisch F and Peng RD (eds.), _Implementing Reproducible Computational Research_.
Chapman and Hall/CRC. ISBN 978-1466561595, <URL:
http://www.crcpress.com/product/isbn/9781466561595>.

Williams GJ (2011). _Data Mining with Rattle and R: The art of excavating data for
knowledge discovery_, series Use R! Springer. <URL:
http://www.amazon.com/gp/product/1441998896/ref=as_li_qf_sp_asin_tl?ie=UTF8&tag=togaware-20&linkCode=as2&camp=217145&creative=399373&creativeASIN=1441998896>.

Hahsler M, Gruen B and Hornik K (2005). “arules - A Computational Environment for Mining
Association Rules and Frequent Item Sets.” _Journal of Statistical Software_, *14*(15),
pp. 1-25. ISSN 1548-7660, <URL: http://www.jstatsoft.org/v14/i15/>.

Feinerer I, Hornik K and Meyer D (2008). “Text Mining Infrastructure in R.” _Journal of
Statistical Software_, *25*(5), pp. 1-54. <URL: http://www.jstatsoft.org/v25/i05/>.

## Tools

R Core Team (2015). _R: A Language and Environment for Statistical Computing_. R
Foundation for Statistical Computing, Vienna, Austria. <URL: http://www.R-project.org/>.

Xie Y (2015). _knitr: A General-Purpose Package for Dynamic Report Generation in R_. R
package version 1.10, <URL: http://yihui.name/knitr/>.

Hahsler M, Buchta C, Gruen B and Hornik K (2014). _arules: Mining Association Rules and
Frequent Itemsets_. R package version 1.1-6, <URL:
http://CRAN.R-project.org/package=arules>.

Hahsler M and Chelluboina S (2014). _arulesViz: Visualizing Association Rules and
Frequent Itemsets_. R package version 1.0-0, <URL:
http://CRAN.R-project.org/package=arulesViz>.

Therneau T, Atkinson B and Ripley B (2015). _rpart: Recursive Partitioning and
Regression Trees_. R package version 4.1-9, <URL:
http://CRAN.R-project.org/package=rpart>.

Feinerer I and Hornik K (2014). _tm: Text Mining Package_. R package version 0.6, <URL:
http://CRAN.R-project.org/package=tm>.

Bastian M., Heymann S., Jacomy M. (2009). Gephi: an open source software for exploring and manipulating networks. International AAAI Conference on Weblogs and Social Media.